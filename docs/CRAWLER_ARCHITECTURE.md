# ğŸ•·ï¸ Xandeum pNode Crawler - Architecture ComplÃ¨te

## ğŸ“‹ Vue d'Ensemble

Le crawler est le **cÅ“ur du systÃ¨me de monitoring**. Il dÃ©couvre, surveille et collecte les donnÃ©es de **tous les pNodes** du rÃ©seau Xandeum (MAINNET et DEVNET).

**ExÃ©cution:** Toutes les **30 minutes** via GitHub Actions  
**Fichier:** `scripts/crawler.ts` (1102 lignes)  
**Workflow:** `.github/workflows/crawler.yml`

---

## ğŸ¯ Objectifs du Crawler

1. **DÃ©couverte rÃ©seau** - Trouver tous les pNodes via gossip + RPC
2. **Collecte de donnÃ©es** - Stats, mÃ©tadonnÃ©es, gÃ©olocalisation
3. **Classification** - MAINNET/DEVNET, status (active/gossip/stale)
4. **Historique** - Tracking des changements dans le temps
5. **Nettoyage** - Supprimer/marquer les "zombie nodes"

---

## ğŸ”„ Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GITHUB ACTIONS                            â”‚
â”‚              Cron: */30 * * * * (every 30 min)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CRAWLER MAIN FUNCTION                       â”‚
â”‚                   scripts/crawler.ts                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼                   â–¼
   PHASE 0             PHASE 1             PHASE 2
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Official   â”‚  â”‚   Network    â”‚  â”‚     Data     â”‚
â”‚  Registries  â”‚  â”‚  Discovery   â”‚  â”‚  Enrichment  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   SUPABASE DATABASE    â”‚
               â”‚  â€¢ pnodes              â”‚
               â”‚  â€¢ pnode_history       â”‚
               â”‚  â€¢ network_metadata    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¡ Phase 0: Official Registries

**Objectif:** RÃ©cupÃ©rer les listes officielles de pNodes depuis les APIs Xandeum

### Sources de DonnÃ©es

1. **MAINNET API**
   ```
   https://mainnet.xandeum.com/get-nodes
   ```

2. **DEVNET API**
   ```
   https://devnet.xandeum.com/get-nodes
   ```

### DonnÃ©es CollectÃ©es

```typescript
interface OfficialNode {
  pubkey: string;      // ClÃ© publique du node
  credits: number;     // CrÃ©dits STOINC accumulÃ©s
  network: "MAINNET" | "DEVNET";
}
```

### FonctionnalitÃ©

- **`fetchOfficialRegistries()`** (dans `lib/official-apis.ts`)
- Retourne:
  - `Set<pubkey>` pour MAINNET et DEVNET
  - `Map<pubkey, credits>` pour les crÃ©dits
  - Compteurs par rÃ©seau

**UtilitÃ©:**
- Classifier les nodes (MAINNET vs DEVNET)
- Calculer le **confidence score** (nodes officiels = plus fiables)
- RÃ©cupÃ©rer les crÃ©dits STOINC

---

## ğŸ” Phase 1: Network Discovery

**Objectif:** DÃ©couvrir TOUS les pNodes du rÃ©seau via propagation

### Algorithme de DÃ©couverte (BFS ParallÃ©lisÃ©)

```
1. START avec bootstrap nodes (config/bootstrap.json)
2. POUR chaque node dÃ©couvert:
   a) Query GOSSIP endpoint (port 5000)
   b) Query RPC endpoint (port 6000/9000)
   c) Extraire les peers (IPs des autres nodes)
3. AJOUTER les nouveaux peers Ã  la queue
4. RÃ‰PÃ‰TER jusqu'Ã  Ã©puisement de la queue
```

### Endpoints InterrogÃ©s

#### 1. **Gossip Endpoint** (Port 5000)
```http
GET http://<IP>:5000/gossip
```

**RÃ©ponse:**
```json
{
  "pnodes": [
    { "ip": "1.2.3.4" },
    { "ip": "5.6.7.8" },
    ...
  ]
}
```

#### 2. **RPC Endpoint** (Port 6000 ou 9000)
```http
POST http://<IP>:6000/rpc
{
  "jsonrpc": "2.0",
  "method": "get-pods",
  "id": 1
}
```

**RÃ©ponse:**
```json
{
  "result": {
    "pods": [
      { "address": "1.2.3.4:6000" },
      { "address": "5.6.7.8:6000" },
      ...
    ]
  }
}
```

### Optimisations de Performance

1. **Batch Processing** - 10 nodes en parallÃ¨le (DISCOVERY_CONCURRENCY)
2. **Port Fallback** - Teste port 6000, puis 9000 si Ã©chec
3. **Timeouts** - 5000ms max par requÃªte
4. **Deduplication** - Set pour Ã©viter les doublons

### RÃ©sultat Phase 1

- Liste complÃ¨te des IPs dÃ©couverts
- ~300+ nodes typiquement (MAINNET + DEVNET)

---

## ğŸ“Š Phase 2: Data Enrichment

**Objectif:** Collecter les mÃ©tadonnÃ©es et stats pour chaque node

### Ã‰tape 2.1: MÃ©tadonnÃ©es (get-pods-with-stats)

**Endpoint RPC:**
```http
POST http://<IP>:6000/rpc
{
  "jsonrpc": "2.0",
  "method": "get-pods-with-stats",
  "id": 1
}
```

**DonnÃ©es CollectÃ©es:**
```typescript
interface PodMetadata {
  address: string;           // IP:port
  version: string;           // ex: "0.15.3"
  pubkey: string;            // ClÃ© publique
  storage_committed: number; // GB committÃ©s
  storage_used: number;      // GB utilisÃ©s
  uptime: number;            // Secondes
  is_public: boolean;        // Node public?
  rpc_port: number;          // Port RPC
  last_seen_timestamp: number; // Dernier vu (gossip)
}
```

**Batch Processing:**
- 100 nodes en parallÃ¨le (METADATA_BATCH_SIZE)
- Construction de Maps pour accÃ¨s O(1):
  - `versionMap`
  - `pubkeyMap`
  - `storageCommittedMap`
  - `storageUsedMap`
  - `isPublicMap`
  - `rpcPortMap`
  - `lastSeenGossipMap`
  - `uptimeGossipMap`

---

### Ã‰tape 2.2: Stats DÃ©taillÃ©es (get-stats)

**Endpoint RPC:**
```http
POST http://<IP>:6000/rpc
{
  "jsonrpc": "2.0",
  "method": "get-stats",
  "id": 1
}
```

**DonnÃ©es CollectÃ©es:**
```typescript
interface PNodeStats {
  active_streams: number;     // Streams actifs
  cpu_percent: number;        // CPU usage %
  current_index: number;      // Index actuel
  file_size: number;          // Taille fichier
  last_updated: number;       // Timestamp
  packets_received: number;   // Paquets reÃ§us
  packets_sent: number;       // Paquets envoyÃ©s
  ram_total: number;          // RAM totale
  ram_used: number;           // RAM utilisÃ©e
  total_bytes: number;        // Bytes totaux
  total_pages: number;        // Pages totales
  uptime: number;             // Uptime en secondes
}
```

**Optimisations:**
- Batch: 100 nodes en parallÃ¨le (BATCH_SIZE)
- Port prioritaire: Utilise `rpc_port` de l'Ã©tape 2.1 si disponible
- Fallback automatique si Ã©chec

**Taux de SuccÃ¨s:**
- ~70-80% des nodes rÃ©pondent Ã  get-stats
- Les autres sont marquÃ©s comme "gossip_only"

---

### Ã‰tape 2.3: GÃ©olocalisation

**Objectif:** DÃ©terminer la localisation gÃ©ographique de chaque IP

#### Providers UtilisÃ©s (avec fallback)

**1. Primary: ipwho.is** (Gratuit)
```http
GET https://ipwho.is/<IP>?fields=success,latitude,longitude,city,country,country_code
```

**2. Fallback 1: ip-api.com** (Gratuit, 45 req/min)
```http
GET http://ip-api.com/json/<IP>?fields=lat,lon,city,country,countryCode
```

**3. Fallback 2: ipapi.co** (Gratuit, rate-limited)
```http
GET https://ipapi.co/<IP>/json/
```

#### SystÃ¨me de Cache Intelligent

```typescript
// Phase 2.3 geolocate only NEW IPs
cachedIps = IPs with existing lat/lng in DB
newIps = IPs without geolocation

// Skip API calls for cachedIps (reuse DB data)
// Only geolocate newIps through rate limiter
```

**Avantages:**
- Ã‰vite les appels API inutiles
- Respecte les rate limits (44 req/min avec Bottleneck)
- AccÃ©lÃ¨re considÃ©rablement le crawl (~90% des IPs sont en cache)

#### Rate Limiting (Bottleneck)

```typescript
const limiter = new Bottleneck({
  maxConcurrent: 1,   // 1 requÃªte Ã  la fois
  minTime: 1350,      // 1.35s entre chaque (~44 req/min)
});
```

**DonnÃ©es CollectÃ©es:**
```typescript
interface GeolocationData {
  lat: number;          // Latitude
  lng: number;          // Longitude
  city: string;         // Ville
  country: string;      // Pays
  country_code: string; // Code ISO (US, FR, etc.)
}
```

---

### Ã‰tape 2.4: Classification & Scoring

#### 1. **Network Classification**

**MÃ©thode:** `networkDetector.classifyNode(pubkey)`

```typescript
if (pubkey in mainnetRegistry) â†’ MAINNET
else if (pubkey in devnetRegistry) â†’ DEVNET
else â†’ UNKNOWN
```

#### 2. **Status Classification**

```typescript
if (has_stats && uptime > 0) â†’ "active"
else if (has_gossip_data) â†’ "gossip_only"
else if (failed_checks >= 2) â†’ "stale"
```

**Types de Status:**
- **active**: Node rÃ©pond au RPC et a des stats
- **gossip_only**: Visible dans gossip mais ne rÃ©pond pas au RPC (souvent privÃ©)
- **stale**: N'a pas rÃ©pondu depuis 2+ crawls

#### 3. **Confidence Score** (0-100)

**Calcul:** `calculateConfidence(node)` dans `lib/confidence-scoring.ts`

**CritÃ¨res:**
- âœ… **Uptime** (35 pts) - Node opÃ©rationnel
- âœ… **Version consensus** (25 pts) - Version majoritaire
- âœ… **Pubkey verification** (20 pts) - Pubkey valide
- âœ… **Official registry** (30 pts) - Dans registre officiel
- âœ… **Storage contribution** (10 pts) - Stockage committÃ©

**Exemple:**
```typescript
Node A: uptime=99%, version=majority, pubkey=valid, official=yes, storage=100GB
â†’ Confidence = 35 + 25 + 20 + 30 + 10 = 120 (plafonnÃ© Ã  100)
```

#### 4. **Credits STOINC**

**Source:** Official registries API

```typescript
const credits = officialRegistries.mainnetCredits.get(pubkey) ||
                officialRegistries.devnetCredits.get(pubkey) ||
                0;
```

---

## ğŸ’¾ Phase 3: Database Save

### Tables Supabase

#### 1. **pnodes** (Table principale)

```sql
CREATE TABLE pnodes (
  id SERIAL PRIMARY KEY,
  ip VARCHAR(45) UNIQUE NOT NULL,
  network network_type,
  status pnode_status,
  version TEXT,
  pubkey TEXT,
  
  -- Stats
  uptime NUMERIC,
  cpu_percent NUMERIC,
  ram_used BIGINT,
  ram_total BIGINT,
  storage_committed NUMERIC,
  storage_used NUMERIC,
  
  -- Geolocation
  lat NUMERIC,
  lng NUMERIC,
  city TEXT,
  country TEXT,
  country_code VARCHAR(2),
  
  -- Scoring
  confidence_score INTEGER,
  credits NUMERIC,
  
  -- Metadata
  last_crawled_at TIMESTAMPTZ,
  failed_checks INTEGER DEFAULT 0,
  last_seen_gossip TIMESTAMPTZ,
  
  -- Manager wallet
  manager_wallet TEXT,
  
  -- Timestamps
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Upsert Strategy:**
```typescript
await supabase
  .from('pnodes')
  .upsert(nodes, { onConflict: 'ip' });
```

- **Conflict:** Si IP existe, mise Ã  jour
- **New:** Si nouvelle IP, insertion

#### 2. **pnode_history** (Historique)

```sql
CREATE TABLE pnode_history (
  id SERIAL PRIMARY KEY,
  ip VARCHAR(45) NOT NULL,
  timestamp TIMESTAMPTZ NOT NULL,
  
  -- Changements trackÃ©s
  status pnode_status,
  uptime NUMERIC,
  cpu_percent NUMERIC,
  confidence_score INTEGER,
  
  -- Index pour queries rapides
  INDEX idx_history_ip_timestamp (ip, timestamp DESC)
);
```

**Politique de rÃ©tention:** 7 jours (via trigger automatique)

**Insertion:**
```typescript
// Seulement si changement significatif
if (needsHistoryEntry(node)) {
  await supabase.from('pnode_history').insert({
    ip: node.ip,
    timestamp: now,
    status: node.status,
    uptime: node.uptime,
    ...
  });
}
```

#### 3. **network_metadata** (Singleton)

```sql
CREATE TABLE network_metadata (
  id INTEGER PRIMARY KEY DEFAULT 1,
  network_total INTEGER,      -- Total nodes dÃ©couverts
  crawled_nodes INTEGER,      -- Nodes crawlÃ©s avec succÃ¨s
  active_nodes INTEGER,       -- Nodes avec status=active
  last_updated TIMESTAMPTZ,
  
  CONSTRAINT single_row CHECK (id = 1)
);
```

**Update:**
```typescript
await supabase.from('network_metadata').upsert({
  id: 1,
  network_total: discoveredIPs.length,
  crawled_nodes: successfulNodes.length,
  active_nodes: activeCount,
  last_updated: new Date().toISOString()
}, { onConflict: 'id' });
```

---

## ğŸ§¹ Phase 4: Zombie Cleanup

**Objectif:** Nettoyer les nodes "morts" ou inaccessibles

### DÃ©finition d'un Zombie

**Logique hybride:**

```typescript
const isZombie =
  (failed_checks >= 2 && !hasGossipData) ||  // Vraiment mort
  (failed_checks >= 4 && hasGossipData);     // ProblÃ¨mes persistants
```

**`hasGossipData`:**
- Node a une version connue
- Node a un pubkey connu
- Node a du storage committÃ©

### Comportement (Configurable)

#### Mode 1: DELETE (Default)
```typescript
KEEP_ZOMBIES = false (default)

â†’ DELETE zombies from pnodes
```

#### Mode 2: STALE (Keep for coverage)
```typescript
KEEP_ZOMBIES = true

â†’ UPDATE status='stale' WHERE ip IN (zombies)
```

**Avantages du mode STALE:**
- Garde la couverture rÃ©seau complÃ¨te
- Permet d'analyser les patterns de dÃ©faillance
- Historique prÃ©servÃ©

### Exclusions

**Private nodes EXCLUS du cleanup:**
```typescript
if (ip.startsWith('PRIVATE-')) {
  // Ne PAS marquer comme zombie
  // Ces nodes sont injoignables par design
}
```

---

## âš™ï¸ Configuration & Variables d'Environnement

### Variables Requises

```bash
# Supabase (Requis)
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=xxx

# Geolocation (Optionnel, amÃ©liore rate limits)
IPWHO_API_KEY=xxx
```

### Variables Optionnelles (Flags)

```bash
# Keep zombies as "stale" instead of deleting
CRAWLER_KEEP_ZOMBIES=1

# Enable port fallbacks (6000, 9000)
CRAWLER_PORT_FALLBACKS=1

# Environment
NODE_ENV=production
```

### Bootstrap Nodes

**Fichier:** `config/bootstrap.json`

```json
{
  "seeds": [
    "1.2.3.4",
    "5.6.7.8",
    "9.10.11.12"
  ]
}
```

**RÃ´le:** Points d'entrÃ©e initiaux pour la dÃ©couverte rÃ©seau

---

## ğŸ“Š MÃ©triques & Logging

### Statistiques TrackÃ©es

Le crawler affiche en temps rÃ©el:

```
ğŸ“¡ get-stats calls: 300
   âœ… Success: 240 (80%)
   âŒ Failed: 60 (20%)
   
ğŸ“Š Port usage:
   - Port 6000: 220 nodes (91.6%)
   - Port 9000: 20 nodes (8.4%)

ğŸ“¡ get-pods-with-stats calls: 300
   âœ… Success (pods>0): 285
   ğŸ“¦ Total pods returned: 8550 (avg: 30 pods/node)

ğŸ“ Geolocation:
   ğŸ“Š Cached: 270 IPs (90%)
   ğŸ†• New: 30 IPs (10%)
   â±ï¸  Time: 45.2s

ğŸ’¾ Database:
   âœ… Saved: 300 nodes
   ğŸ“ History: 180 entries
   ğŸ§¹ Zombies: 5 marked as stale
```

### Logs DÃ©taillÃ©s

Le crawler produit des logs structurÃ©s:

```
ğŸ•·ï¸ Starting network crawl...
================================================

ğŸ“¡ PHASE 0: Fetching Official Registries...
âœ… MAINNET: 32 official nodes
âœ… DEVNET: 268 official nodes

ğŸŒ Initializing network detector...
âœ… Network registry loaded: 32 MAINNET, 268 DEVNET

ğŸ” PHASE 1: Network Discovery...
ğŸ” Querying 10 nodes in parallel...
âœ… Discovery complete. Found 305 unique nodes

ğŸ“Š PHASE 2: Data Enrichment...
ğŸ“¡ Fetching metadata...
  Fetched metadata for 100/305 nodes...
  Fetched metadata for 200/305 nodes...
  Fetched metadata for 305/305 nodes...
âœ… Metadata discovery complete

ğŸ“Š Fetching stats...
  Fetched stats for 100/305 nodes...
  Fetched stats for 200/305 nodes...
  Fetched stats for 305/305 nodes...

ğŸ“ Geolocating 305 IPs...
   ğŸ“Š Cache: 275 cached, 30 new IPs
   âœ… Geolocation complete (42.3s)

ğŸ’¾ PHASE 3: Database Save...
ğŸ’¾ Saving 305 unique nodes...
âœ… Successfully saved pnodes data
ğŸ“Š Updating network metadata: 305 total, 305 crawled, 240 active
âœ… Network metadata updated
ğŸ’¾ Saving 180 history records...
âœ… Successfully saved history data

ğŸ§¹ PHASE 4: Zombie Cleanup...
ğŸ—‘ï¸  Found 5 zombie nodes
âœ… Marked 5 nodes as stale

================================================
âœ… Crawl completed successfully!
Total time: 2m 15s
```

---

## ğŸ”§ Optimisations de Performance

### 1. Parallelisation Massive

```typescript
// Discovery: 10 concurrent
const DISCOVERY_CONCURRENCY = 10;

// Metadata: 100 concurrent
const METADATA_BATCH_SIZE = 100;

// Stats: 100 concurrent
const BATCH_SIZE = 100;
```

### 2. Rate Limiting Intelligent

```typescript
// Geolocation: 44 req/min pour respecter ip-api.com
const limiter = new Bottleneck({
  maxConcurrent: 1,
  minTime: 1350, // 1.35s = ~44 req/min
});
```

### 3. Caching Agressif

- **GÃ©olocalisation:** RÃ©utilise 90% des donnÃ©es existantes
- **Metadata:** Maps en mÃ©moire pour O(1) lookup
- **Network registry:** ChargÃ© une fois au dÃ©but

### 4. Timeouts Courts

```typescript
const TIMEOUT = 5000; // 5s max par requÃªte
```

**Avantages:**
- Ã‰vite de bloquer sur nodes lents
- Crawl complet en ~2-3 minutes

### 5. Port Fallback Strategy

```typescript
// Essaie d'abord le port prÃ©fÃ©rÃ© du node
const preferred = rpcPortMap.get(ip);
const ports = preferred 
  ? [preferred, ...DEFAULT_PRPC_PORTS.filter(p => p !== preferred)]
  : DEFAULT_PRPC_PORTS; // [6000, 9000]
```

---

## ğŸš€ DÃ©ploiement & ExÃ©cution

### GitHub Actions (Production)

**Fichier:** `.github/workflows/crawler.yml`

```yaml
on:
  schedule:
    - cron: '*/30 * * * *'  # Toutes les 30 minutes
  workflow_dispatch:        # Manual trigger
```

**Steps:**
1. Checkout code
2. Setup Node.js 20
3. Install dependencies (`npm ci`)
4. Run crawler (`npx tsx scripts/crawler.ts`)
5. Save daily snapshot (`scripts/save-daily-snapshot.ts`)

**Timeout:** 15 minutes (sÃ©curitÃ©)

### ExÃ©cution Locale

```bash
# Setup env
cp .env.example .env.local
# Edit .env.local avec tes credentials

# Run crawler
npx tsx scripts/crawler.ts

# Avec options
CRAWLER_KEEP_ZOMBIES=1 npx tsx scripts/crawler.ts
CRAWLER_PORT_FALLBACKS=1 npx tsx scripts/crawler.ts
```

### Manual Trigger (GitHub UI)

1. Va sur **Actions** tab
2. SÃ©lectionne **Xandeum pNodes Crawler**
3. Clique **Run workflow**
4. Options:
   - âœ… Enable port fallbacks (6000/9000)
   - âœ… Keep zombies (stale instead of delete)

---

## ğŸ“ˆ AmÃ©liorations Futures

### Court Terme
- [ ] **WebSocket real-time** au lieu de polling 30 min
- [ ] **Prometheus metrics** export pour monitoring
- [ ] **Alerting** sur anomalies rÃ©seau
- [ ] **Retry logic** avec backoff exponentiel

### Moyen Terme
- [ ] **Distributed crawling** pour scale horizontale
- [ ] **Node health predictions** avec ML
- [ ] **Geographic routing** optimisÃ©
- [ ] **Custom scoring** configurable par utilisateur

### Long Terme
- [ ] **Full mesh topology** mapping
- [ ] **Network simulation** pour prÃ©dire comportement
- [ ] **Auto-tuning** des paramÃ¨tres de crawl
- [ ] **Multi-chain support** (autres blockchains)

---

## ğŸ› Troubleshooting

### ProblÃ¨me: Crawler timeout (>15 min)

**Causes:**
- Trop de nouveaux nodes Ã  gÃ©olocaliser
- Rate limits sur APIs gÃ©olocation
- Nodes trÃ¨s lents qui bloquent

**Solutions:**
```bash
# Augmenter le cache geolocation
# RÃ©duire BATCH_SIZE si rate limits
# Augmenter timeout dans workflow.yml
```

### ProblÃ¨me: Beaucoup de "failed_checks"

**Causes:**
- Nodes rÃ©ellement down
- Firewall bloquant crawler
- Mauvaises IPs dans bootstrap

**Solutions:**
```bash
# Activer KEEP_ZOMBIES=1 pour analyser
# VÃ©rifier bootstrap.json
# Tester manuellement avec curl
curl http://<IP>:5000/gossip
curl -X POST http://<IP>:6000/rpc \
  -d '{"jsonrpc":"2.0","method":"get-stats","id":1}'
```

### ProblÃ¨me: GÃ©olocalisation bloquÃ©e

**Causes:**
- Rate limits ip-api.com (45 req/min)
- IPs invalides/privÃ©es
- Provider API down

**Solutions:**
```bash
# VÃ©rifier que 90%+ sont en cache
# Bottleneck devrait gÃ©rer automatiquement
# Fallback vers ipapi.co si besoin
```

---

## ğŸ“š Fichiers LiÃ©s

| Fichier | RÃ´le |
|---------|------|
| `scripts/crawler.ts` | Crawler principal (1102 lignes) |
| `scripts/crawler-types.ts` | Types TypeScript |
| `scripts/save-daily-snapshot.ts` | Snapshot quotidien |
| `.github/workflows/crawler.yml` | GitHub Actions config |
| `config/bootstrap.json` | Bootstrap nodes |
| `lib/official-apis.ts` | Fetch registries officiels |
| `lib/network-detector.ts` | Classification MAINNET/DEVNET |
| `lib/confidence-scoring.ts` | Calcul confidence score |

---

**DerniÃ¨re mise Ã  jour:** 2026-01-21  
**Version:** 2.0 (Manager Board integrated)
